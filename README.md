# CLI client for ETL

This project implements a CLI client for performing ETL on Solar and Wind APIs and storing results on disk in the specified output format.

## Prerequisites

This project requires Python 3.11 or later to support new `asyncio` features.

`ğŸ’¡ The exact version used for this project is Python3.11.4`

## Installation

- Install Python 3.11 using [this](https://www.python.org/downloads/release/python-3114/) link.
- Create a virtual environment

```shell
$ python3.11 -m venv venv
```

- Activate the virtual environment

```shell
$ source venv/bin/activate
```

- Install dependencies

```shell
$ pip install -r requirements.txt
```

## Usage and Examples

- This project uses `argparse` to parse command line arguments. To see the list of available commands, run:

```
âš ï¸ All commands should be run from the root directory of the project.
```

```shell
$ python3.11 cli.py -h

usage: cli.py [-h] --source {wind,solar,all} [--output-format {csv,parquet}] [--output-path OUTPUT_PATH] [--combine-output] [--start-date START_DATE] [--end-date END_DATE]
              [--lookback-days LOOKBACK_DAYS]

CLI client for running ETL jobs

options:
  -h, --help            show this help message and exit
  --source {wind,solar,all}, -s {wind,solar,all}
                        Data source to fetch. Specify 'all' to fetch both wind and solar data.
  --output-format {csv,parquet}, -f {csv,parquet}
                        Output format for the result of ETL jobs. Defaults to 'parquet'.
  --output-path OUTPUT_PATH, -o OUTPUT_PATH
                        The path to the directory where to output file should be saved. Defaults to './output' directory.
  --combine-output, -co
                        Combine the wind and solar data generated by ETL jobs into a single file. Defaults to True.
  --start-date START_DATE, -sd START_DATE
                        Start date in YYYY-MM-DD format
  --end-date END_DATE, -e END_DATE
                        End date in YYYY-MM-DD format
  --lookback-days LOOKBACK_DAYS, -d LOOKBACK_DAYS
                        Number of days to look back

```

- Run ETL jobs for solar and wind APIs for the last week and store the result in `CSV` format in the `./output` directory. 

```shell
$ python3.11 cli.py --source all  --output-format parquet --lookback-days 7

[2023-09-27 13:10:39] [INFO] [__main__] - ------------------------------ Starting ETL process for all data ------------------------------
[2023-09-27 13:10:39] [INFO] [etl.wind] - Extracting wind data for date range 2023-09-19 to 2023-09-26
[2023-09-27 13:10:39] [INFO] [etl.solar] - Extracting solar data for date range 2023-09-19 to 2023-09-26
[2023-09-27 13:10:40] [INFO] [api] - Finished fetching data for date range [2023-09-19 to 2023-09-26] from [http://localhost:8000/{date}/renewables/solargen.json?api_key=ADU8S67Ddy!d7f?]
[2023-09-27 13:10:40] [INFO] [etl.solar] - Transforming solar data for date range 2023-09-19 to 2023-09-26
[2023-09-27 13:10:40] [INFO] [etl.solar] - Loading solar data for date range 2023-09-19 to 2023-09-26
[2023-09-27 13:10:40] [INFO] [utils] - Saved ETL output for solar data to [/Users/gaurav/projects/etl_client/output/solar_1695081600_1695686400.parquet]
[2023-09-27 13:10:42] [INFO] [api] - Finished fetching data for date range [2023-09-19 to 2023-09-26] from [http://localhost:8000/{date}/renewables/windgen.csv?api_key=ADU8S67Ddy!d7f?]
[2023-09-27 13:10:42] [INFO] [etl.wind] - Transforming wind data for date range 2023-09-19 to 2023-09-26
[2023-09-27 13:10:42] [INFO] [etl.wind] - Loading wind data for date range 2023-09-19 to 2023-09-26
[2023-09-27 13:10:42] [INFO] [utils] - Saved ETL output for wind data to [/Users/gaurav/projects/etl_client/output/wind_1695081600_1695686400.parquet]
[2023-09-27 13:10:42] [INFO] [__main__] - Combining wind and solar data for date range 2023-09-19 to 2023-09-26
[2023-09-27 13:10:42] [INFO] [utils] - Saved ETL output for combined data to [/Users/gaurav/projects/etl_client/output/combined_1695081600_1695686400.parquet]
[2023-09-27 13:10:42] [INFO] [__main__] - ------------------------------ ETL process completed in 2.285 seconds ------------------------------

```

- Run ETL job for wind API for the last 7 days and store the result in `parquet` format in `./output` directory

```shell
$ python3.11 cli.py --source wind --output-format parquet --output-path ./output --lookback-days 7

[2023-09-27 12:17:28] [INFO] [__main__] - ------------------------------ Starting ETL process for wind data ------------------------------
[2023-09-27 12:17:28] [INFO] [etl.wind] - Extracting wind data for date range 2023-09-19 to 2023-09-26
[2023-09-27 12:17:29] [INFO] [api] - Finished fetching data for date range [2023-09-19 to 2023-09-26] from [http://localhost:8000/{date}/renewables/windgen.csv?api_key=ADU8S67Ddy!d7f?]
[2023-09-27 12:17:29] [INFO] [etl.wind] - Transforming wind data for date range 2023-09-19 to 2023-09-26
[2023-09-27 12:17:29] [INFO] [etl.wind] - Loading wind data for date range 2023-09-19 to 2023-09-26
[2023-09-27 12:17:29] [INFO] [utils] - Saved ETL output for wind data to [output/wind_1695081600_1695686400.parquet]
[2023-09-27 12:17:29] [INFO] [__main__] - ------------------------------ ETL process completed in 1.117 seconds ------------------------------

```


- You can also specify the start and end date for the ETL jobs and combine the output into a single file.

```shell
$ python3.11 cli.py --source all  --output-format csv --end-date 2023-09-28 --start-date 2023-09-01 --combine-output
```

## Running Tests

This project uses `pytest` for running tests. To run all tests, use in the root directory of the project:

```shell
$ pytest
```

## Logging

This project uses the `logging` module for logging. The default log level is `INFO`. To change the log level, set the `LOG_LEVEL` environment variable to one of the following values: `DEBUG`, `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.

## Design

### Project Structure

- `api.py` - Async API client for concurrently fetching data from the API data source.
- `cli.py` - CLI client for running ETL jobs.
- `etl` - ETL jobs for solar and wind data.
- `pytest.ini` - Contains the `pytest` configuration.
- `requirements.txt` - Third-part dependencies for the project.
- `settings.py` - Global project settings.
- `.env` - Environment variables for the project. This project uses the `python-decouple` module to load environment variables from the `.env` file and update the settings. Typically, this file should not be checked into version control, but for the sake of simplicity, I have included it in the project.
- `tests` - Unit tests for the project.
- `utils.py` - Common util functions used across the project.

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â”œâ”€â”€ api.py
â”œâ”€â”€ cli.py
â”œâ”€â”€ etl
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ base.py
â”‚Â Â  â”œâ”€â”€ solar.py
â”‚Â Â  â””â”€â”€ wind.py
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ settings.py
â”œâ”€â”€ tests
â”‚Â Â  â”œâ”€â”€ test_api.py
â”‚Â Â  â”œâ”€â”€ test_cli.py
â”‚Â Â  â”œâ”€â”€ test_etl_solar.py
â”‚Â Â  â”œâ”€â”€ test_etl_wind.py
â”‚Â Â  â””â”€â”€ test_utils.py
â”œâ”€â”€ utils.py
```
### Design Overview
- This project uses `asyncio` to run ETL jobs concurrently.
- The ETL jobs are implemented as classes that inherit from the `ETLBase` class. This allows us to add new ETL jobs in the future easily. The changes in the data at each step of the ETL are tracked in memory.
- CLI client triggers the ETL jobs for one or more sources (solar, wind, all) and specifies the date range. If a date range is not provided, the ETL will be performed for the last week by default.
- The `cli.main()` starts the `asyncio` event loop and schedules the `cli.etl_main()` with arguments specified on the command-line.
- `cli.etl_main()` creates required instances of ETL jobs and runs them concurrently. 


### ETL Steps
- `Extract` : `AsynDataFetcher` fetches the data from Solar and Wind API. The request throttling is handled using exponential backoff with jitter. The extracted data is stored as-is in memory.
- `Transform`: Takes the extracted data as input, and runs transformations.
- `Load`: Takes the transformed data and stores it in the specified format.
  
### ETL Output
Data generated by the ETL jobs are stored in the `--output-path`. The output files are created in the `./output` directory by default with the following format `<source>_<start_epoch>_<end_epoch>.<output_format>`.

```
combined_1695081600_1695686400.parquet 
solar_1695081600_1695686400.parquet    
wind_1695081600_1695686400.parquet  
```

Two output formats are supported `partquet` and `csv`. 

```
âš ï¸ I have added support for the `CSV` format primarily for readability and easier debugging during development.
```

I chose `parquet` as the default format because of several reasons:
1. Preserves data type and the schema of the data
2. Faster Read/Write performance for large files
3. Smaller disk footprint because of compression
4. Widely used in data-analysis
5. Supported in Pandas using `pyarrow` or `fastparquet` as the engine. For other good alternatives, like `ORC`, Pandas does not have in-built support. 
   
## Assumptions

- The ETL client is designed to run on a single machine.
- The streaming response for the wind API is small enough to be loaded into memory.
- The data at each step in the ETL process is small enough to be loaded into memory.

## Future Improvements

- Handle streaming response for the wind API.
- While the Extract step for ETL jobs are concurrent, the Transform and Load steps are not.
- The Transform step uses `pandas` to transform the data, which is not designed to be concurrent. We can use `dask` to parallelize the Transform step. The Load step is also not concurrent, but we can use `aiofiles` to write the data to disk in chunks. The other alternative is to use `concurrent.futures.ThreadPoolExecutor` for I/O bound tasks and `concurrent.futures.ProcessPoolExecutor` for CPU-bound tasks.
- Add more unit tests and integration tests for the project.
- Separate development dependencies like `pytest`, `pytest-asyncio`, and `pre-commit` from production dependencies using separate `requirements.txt` or more advanced dependency management tools like `poetry`.
